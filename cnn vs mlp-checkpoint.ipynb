{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIO1ynX7avZd"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pading_the_grain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e8c7bc6f7a11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpading_the_grain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mPATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ENLARGED_TRAINABLE_DATASET'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pading_the_grain'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "import os\n",
    "import numpy as np\n",
    "import pading_the_grain\n",
    "from lib.utils import *\n",
    "PATH = 'ENLARGED_TRAINABLE_DATASET'\n",
    "label_encoder = {}\n",
    "label_decoder = {}\n",
    "w=0.90\n",
    "i = 0\n",
    "for grain_type in os.listdir(PATH):\n",
    "  \n",
    "\n",
    "  \n",
    "  label_encoder[str(grain_type)] = i\n",
    "  label_decoder[i] = grain_type\n",
    "  i+=1\n",
    "ac=[]\n",
    "print(label_encoder)\n",
    "print(label_decoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wyjEH6-5bbFz"
   },
   "outputs": [],
   "source": [
    "accuracy=[]\n",
    "partition = {}\n",
    "partition['train'] = []\n",
    "partition['validation'] = []\n",
    "\n",
    "\n",
    "label = {}\n",
    "\n",
    "\n",
    "for grain_type in os.listdir(PATH):\n",
    "  \n",
    "\n",
    "\n",
    "    \n",
    "  type_path = os.path.join(PATH,grain_type)\n",
    "  \n",
    "  type_list = os.listdir(type_path)\n",
    "  \n",
    "  \n",
    "  np.random.seed(10)\n",
    "  \n",
    "  fixed_type_list = np.random.choice(type_list, size=len(type_list), replace=False)\n",
    "  \n",
    "  train_list = fixed_type_list[3000:]\n",
    "  validation_list = fixed_type_list[:3000]\n",
    "  \n",
    "  for item in train_list:\n",
    "    item_path = os.path.join(type_path, item)\n",
    "    partition['train'].append(item_path)\n",
    "    label[item_path] = label_encoder[grain_type] \n",
    "    \n",
    "  for item in validation_list:\n",
    "    item_path = os.path.join(type_path, item)\n",
    "    partition['validation'].append(item_path)\n",
    "    label[item_path] = label_encoder[grain_type]\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uh3x7oy-cHvo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import imutils\n",
    "\n",
    "no_classes = 8\n",
    " \n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.all_utils.Sequence):\n",
    "    \n",
    "    def __init__(self, list_IDs, labels, batch_size=64, dim=(128,128), n_channels=3,\n",
    "                 n_classes=no_classes, shuffle=False):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        \n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        \n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "         \n",
    "      \n",
    "\n",
    "    def __data_generation(self,list_IDs_temp):\n",
    "        \n",
    "        \n",
    "        X = np.empty((self.batch_size, self.dim[0], self.dim[1], self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "       \n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            \n",
    "            try:\n",
    "              _ = (cv2.imread(ID).shape!=(128,128,3))\n",
    "            except:\n",
    "              continue\n",
    "            try:\n",
    "              X[i,] = cv2.imread(ID)*(1.0/255.0)\n",
    "              \n",
    "            except:\n",
    "              continue\n",
    "                \n",
    "            \n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYboX3wZdhDr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "training_generator = DataGenerator(partition['train'], label)\n",
    "validation_generator = DataGenerator(partition['validation'], label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iokdA4KxdwSi"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from keras.layers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "model=Sequential()\n",
    "input_shape=(128,128,3)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(6, (5, 5), input_shape=input_shape, padding='same', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "img = mpimg.imread('images/predicted_img.jpg')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()\n",
    "model.add(Conv2D(16, (5, 5),padding='same', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(no_classes, activation = 'softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "img = mpimg.imread('images/predicted_img.jpg')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import imutils\n",
    "import math\n",
    "\n",
    "no_classes = 8\n",
    "\n",
    "def generate_data(ID):\n",
    "\n",
    "    img = cv2.imread(ID)\n",
    "    img_grey = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    canny_img = cv2.Canny(img_grey, 100, 100)\n",
    "      \n",
    "    cnts = cv2.findContours(canny_img.copy(),  cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "      \n",
    "    for c in cnts:\n",
    "      rect = cv2.minAreaRect(c)\n",
    "      ((_, _), (p,q), _) = rect\n",
    "      if(p>=q):\n",
    "          length = p\n",
    "          width = q\n",
    "      else:\n",
    "          length = q\n",
    "          width = p\n",
    "\n",
    "      area = cv2.contourArea(cnts[0])\n",
    "      perimeter = cv2.arcLength(cnts[0], closed=True)\n",
    "      \n",
    "      try:\n",
    "        (_,_), (majax, minax), _ = cv2.fitEllipse(cnts[0])\n",
    "        if(ax1>=ax2):\n",
    "          majax = ax1\n",
    "          minax = ax2\n",
    "        else:\n",
    "          majax = ax2\n",
    "          minax = ax1\n",
    "        return [area/perimeter, area/(area+perimeter), maxax/minax, area/(length*width), math.sqrt(4*area/math.pi), area/(majax*minax)]\n",
    "      \n",
    "      except:\n",
    "        \n",
    "        return [area/perimeter, area/(area+perimeter), length/width, area/(length*width), math.sqrt(4*area/math.pi), area/(length*width)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(image_path):\n",
    "  \n",
    "  k = np.expand_dims(cv2.imread(image_path)*(1.0/255.0), axis=0)\n",
    "  p = model.predict(k)\n",
    "  p = np.argmax(p)\n",
    "  \n",
    "  return label_decoder[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "PATH = 'ENLARGED_TRAINABLE_DATASET'\n",
    "\n",
    "for grain_type in [os.listdir(PATH)[0]]:\n",
    "  \n",
    "  print(f\"Collecting type {grain_type}\")\n",
    "    \n",
    "  type_path = os.path.join(PATH,grain_type)\n",
    "  \n",
    "  type_list = os.listdir(type_path)\n",
    " \n",
    "  np.random.seed(10)\n",
    "  fixed_type_list = np.random.choice(type_list, size=len(type_list), replace=False)\n",
    "\n",
    "  i = 0 \n",
    "  for item in fixed_type_list:\n",
    "    \n",
    "    item_path = os.path.join(type_path, item)\n",
    "    try:\n",
    "      dat = generate_data(item_path)\n",
    "      i+=1\n",
    "    except:\n",
    "      continue\n",
    "      \n",
    "    if(i%1000==0):\n",
    "      print(i) \n",
    "    \n",
    "    \n",
    "    if(i<=3000):\n",
    "      x_test.append(dat)\n",
    "      y_test.append(label_encoder[grain_type])\n",
    "    #print(\"Done with Validation\")\n",
    "    \n",
    "    else:\n",
    "      x_train.append(dat)\n",
    "      y_train.append(label_encoder[grain_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X,y=scaler_transform(training_generator)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier \n",
    "model = MLPClassifier(random_state=1, max_iter=300)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "ac.append(accuracy_score(model,y_test,sample_weight=1)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(20, input_dim=4, activation='relu'))\n",
    "model.add(layers.Dense(10,  activation='tanh'))\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam')\n",
    "\n",
    "classifier_nn = model.fit(X_train,y_train,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=15)\n",
    "ac.append(accuracy_score(model,y_test,sample_weight=0.2)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "plt.style.use('seaborn')\n",
    "x=['MLP','CNN']\n",
    "y=ac\n",
    "ax=sns.barplot(x,y)\n",
    "ax.set_title('Accuracy comparison')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_ylim(80,100)\n",
    "import pandas as pd\n",
    "data={'Agorithms':x,\n",
    "     \"accuracy\":ac}\n",
    "df=pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Rice Grain Analysis.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
